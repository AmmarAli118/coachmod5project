{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score #classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('./data/X_train_pickle.pkl')\n",
    "X_test = pd.read_pickle('./data/X_test_pickle.pkl')\n",
    "y_train = pd.read_pickle('./data/y_train_pickle.pkl')\n",
    "y_test = pd.read_pickle('./data/y_test_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing to Hyper tune RFC using GridSearchCV\n",
    "cv = 5                             # Set how many cross validations you would like.\n",
    "est_range = list(range(120,161,5)) # Set the range of estimators.\n",
    "depth_range = list(range(16,22))   # Set the range of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This only needs to be run on the first use of this notebook, or if changes have been made to the variables above.\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = [                     # GridSearchCV params requires a 'list', so we created a dictionary within the list to pass multiple params.\n",
    "    {'n_estimators': est_range, \n",
    "     'max_depth': depth_range}\n",
    "] \n",
    "\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=cv,          # Just passing in the variables declared above\n",
    "                          scoring='neg_mean_squared_error', # base scoring on the NMSE - higher return values are better than lower return values\n",
    "                          return_train_score=True,          # \n",
    "                          verbose=50)                       # verbose > 0 gives us a progress bar to check on.\n",
    "\n",
    "# running a grid search through range of estimators and range of depth resulting in 48 fit tests.\n",
    "grid_search.fit(X_train, y_train) \n",
    "%store grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best depth and estimator params are: {'max_depth': 19, 'n_estimators': 140}\n",
      "Best Overall RFC parameters is: \n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=19, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=140,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "%store -r grid_search\n",
    "print(\"Best depth and estimator params are:\", grid_search.best_params_)\n",
    "print(\"Best Overall RFC parameters is: \\n\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After hyper tuning the RF model, the best accuracy we could compute was 85.21 with a f1 score of 66.44\n",
      "Stored 'rf' (dict)\n"
     ]
    }
   ],
   "source": [
    "# predicting y hat\n",
    "rfc_pred = grid_search.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "RF_accuracy = round(accuracy_score(y_test, rfc_pred)*100, 2)\n",
    "\n",
    "# checking F1 Score\n",
    "RF_f1 = round(f1_score(y_test, rfc_pred)*100, 2)\n",
    "\n",
    "# Storing RF scores for comparisons.\n",
    "rf = {\n",
    "    'accuracy': RF_accuracy,\n",
    "    'f1': RF_f1\n",
    "}\n",
    "\n",
    "print(\"After hyper tuning the RF model, the best accuracy we could compute was {} with a f1 score of {}\".format(RF_accuracy, RF_f1))\n",
    "%store rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
