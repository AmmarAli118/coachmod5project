{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('./data/X_train_pickle.pkl')\n",
    "X_test = pd.read_pickle('./data/X_test_pickle.pkl')\n",
    "y_train = pd.read_pickle('./data/y_train_pickle.pkl')\n",
    "y_test = pd.read_pickle('./data/y_test_pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyper tuning RFC within a range of degrees\n",
    "cv = 5                             # Set how many cross validations you would like.\n",
    "est_range = list(range(120,161,5)) # Set the range of estimators.\n",
    "depth_range = list(range(16,22))   # Set the range of depth.\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "param_grid = [\n",
    "    {'n_estimators': est_range, \n",
    "     'max_depth': depth_range}\n",
    "] \n",
    "\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=cv,          # Just passing in the variables declared above\n",
    "                          scoring='neg_mean_squared_error', # base scoring on the NMSE - higher return values are better than lower return values\n",
    "                          return_train_score=True,          # \n",
    "                          verbose=50)                       # verbose > 0 gives us a progress bar to check on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run of the mill fit method with training data.\n",
    "grid_search.fit(X_train, y_train) \n",
    "%store grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 19, 'n_estimators': 140}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r grid_search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=19, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=140,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After hyper tuning the RF model, the best accuracy we could compute was 85.21 with a f1 score of 66.44\n",
      "Stored 'rf' (dict)\n"
     ]
    }
   ],
   "source": [
    "# predicting y hat\n",
    "rfc_pred = grid_search.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "acc_score = accuracy_score(y_test, rfc_pred)\n",
    "RF_accuracy = round(acc_score*100, 2)\n",
    "\n",
    "# checking F1 Score\n",
    "f1_sc = f1_score(y_test, rfc_pred)\n",
    "RF_f1 = round(f1_sc*100, 2)\n",
    "\n",
    "print(\"After hyper tuning the RF model, the best accuracy we could compute was {} with a f1 score of {}\".format(RF_accuracy, RF_f1))\n",
    "\n",
    "rf = {\n",
    "    'accuracy': RF_accuracy,\n",
    "    'f1': RF_f1\n",
    "}\n",
    "%store rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
