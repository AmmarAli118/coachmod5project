{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('./data/X_train_pickle.pkl')\n",
    "X_test = pd.read_pickle('./data/X_test_pickle.pkl')\n",
    "y_train = pd.read_pickle('./data/y_train_pickle.pkl')\n",
    "y_test = pd.read_pickle('./data/y_test_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='Random Forest')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=21, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=135,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=0, C=.01)\n",
    "\n",
    "svm = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=1, gamma='auto_deprecated',\n",
    "    kernel='poly', max_iter=-1, probability=True, random_state=None,\n",
    "    shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
    "                     weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [rf,lr,svm,knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=21, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=135,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "# Train the models and record the results\n",
    "for cls in model_list:\n",
    "    print(cls)\n",
    "    model = cls.fit(X_train, y_train)\n",
    "    yproba = model.predict_proba(X_test)[::,1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "    auc = roc_auc_score(y_test, yproba)\n",
    "    print('AUC = {}'.format(auc))\n",
    "    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc}, ignore_index=True)\n",
    "\n",
    "# Set name of the classifiers as index labels\n",
    "result_table.set_index('classifiers', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in result_table.index:\n",
    "    print(i)\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource for plotting ROC curve\n",
    ">https://abdalimran.github.io/2019-06-01/Drawing-multiple-ROC-Curves-in-a-single-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "\n",
    "comparison_dictionary = {\n",
    "    'accuracy':{\n",
    "        'KNN': knn['accuracy'],\n",
    "        'LR': lr['accuracy'],\n",
    "        'RF': rf['accuracy'],\n",
    "        'SVM': svm['accuracy'],\n",
    "    },\n",
    "    'f1':{\n",
    "        'KNN': knn['f1'],\n",
    "        'LR': lr['f1'],\n",
    "        'RF': rf['f1'],\n",
    "        'SVM': svm['f1'],\n",
    "    }\n",
    "}\n",
    "\n",
    "# compares accuracy between models\n",
    "high_accuracy = 0\n",
    "best_model = None\n",
    "for model, accuracy in comparison_dictionary['accuracy'].items():\n",
    "    if accuracy > high_accuracy:\n",
    "        high_accuracy = accuracy\n",
    "        best_model = model\n",
    "print(\"The best accuracy score that we compared was {} and was a result of the {} model\".format(high_accuracy, best_model))    \n",
    "\n",
    "# compares f1 between models\n",
    "high_f1 = 0\n",
    "best_model = None\n",
    "for model, f1 in comparison_dictionary['f1'].items():\n",
    "    if f1 > high_f1:\n",
    "        high_f1 = f1\n",
    "        best_model = model\n",
    "print(\"The best f1 score that we compared was {} and was a result from the {} model\".format(high_f1, best_model))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
